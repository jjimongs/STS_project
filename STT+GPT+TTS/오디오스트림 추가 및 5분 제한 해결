import os
import io
import time
import openai
import pyaudio
import threading
import configparser
from pydub import AudioSegment
from pydub.playback import play
from google.cloud import texttospeech
from google.cloud import speech_v1 as speech
from google.oauth2 import service_account

# ffmpeg와 ffprobe 경로 직접 설정 (환경변수에 추가했는데 잘 되지 않아 직접 설정)
os.environ["PATH"] += os.pathsep + "C:\\Users\\wwsgb\\ffmpeg\\ffmpeg-6.1.1-full_build\\bin"

# 설정파일 로드
config = configparser.ConfigParser()
config.read('config.ini')
openai.api_key = config.get('api_key', 'openai_key')

class Transcriber:
    def __init__(self):
        # STT API 인증 정보 설정 (여기서는 환경변수 대신 서비스 계정 키 파일을 직접 로드하는 방법 사용)
        credentials = service_account.Credentials.from_service_account_file(
            "C:\\Users\\wwsgb\\Json_key\\stt_json_key\\buoyant-mason-412215-845850c6db6c.json")
        self.client = speech.SpeechClient(credentials=credentials)
        self.transcript = ""
        self.last_transcript_time = time.time()
        self.history = []                # 대화 이력 저장
        self.silence_threshold = 5 * 60  # 5분간 대화 없을 시 리셋
        self.running = True
        self.is_speaking = False         # GPT 말하고 있는지에 대한 여부
        self.setup_audio_stream()
    
    def setup_audio_stream(self):
        start_time = time.time()  # 시간 측정 시작
        self.pyaudio_instance = pyaudio.PyAudio()
        self.stream = self.pyaudio_instance.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,
            input=True,
            frames_per_buffer=1024
        )
        end_time = time.time()  # 시간 측정 종료
        print(f"오디오 스트림 시작까지 걸리는 시간: {end_time - start_time} 초")
    
    def restart_audio_stream(self):
        if self.stream.is_active():
            self.stream.stop_stream()
        self.stream.close()
        self.pyaudio_instance.terminate()
        print("오디오 스트림 중지")

    def transcribe_streaming(self):
        while self.running:
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=16000,
                language_code="ko-KR",
                speech_contexts=[{"phrases": ["작동", "그만해", "안녕", "있잖아", "할 말이 있어", "도와줘", "살려줘", "도와줄래", "나 좀 도와줘"]}],  # 커스텀 어휘 리스트 추가
                enable_separate_recognition_per_channel=True  # 멀티채널 인식 활성화
            )

            streaming_config = speech.StreamingRecognitionConfig(config=config, interim_results=True)

            p = pyaudio.PyAudio()
            stream = p.open(format=pyaudio.paInt16, channels=1, rate=16000, input=True, frames_per_buffer=1024)

            def generate_requests():
                start_time = time.time()  # 스트림 시작 시간 기록
                while self.running and time.time() - start_time < 300:  # 5분 이하로만 실행
                    data = stream.read(1024, exception_on_overflow=False)
                    yield speech.StreamingRecognizeRequest(audio_content=data)

            requests = generate_requests()
            responses = self.client.streaming_recognize(streaming_config, requests)

            for response in responses:
                if not response.results:
                    continue

                result = response.results[-1]

                if result.is_final:
                    transcript = result.alternatives[0].transcript
                    
                    if "작동 그만해" in transcript:
                        self.running = False
                        print("프로그램 중단")
                        break
                    
                    if not self.is_speaking:
                        self.transcript = transcript
                        self.last_transcript_time = time.time()
                        self.process_response()
                
            stream.stop_stream()
            stream.close()
            p.terminate()
            
            # 스트림이 종료된 후 자동으로 새 스트림 시작
            if self.running:
                print("5분동안 음성이 들어오지 않았기에 새 스트림을 시작합니다")
                time.sleep(0.25)  # 잠시 대기 후 새 스트림 시작


    def process_response(self):
        print("사용자 질문: {}".format(self.transcript))
        gpt_response = self.get_gpt_response(self.transcript)
        print("GPT 응답: {}".format(gpt_response))
        self.history.append((self.transcript, gpt_response))
        self.text_to_speech(gpt_response)

    def text_to_speech(self, text):
        self.is_speaking = True
        self.restart_audio_stream()
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:\\Users\\wwsgb\\Json_key\\tts_json_key\\buoyant-mason-412215-1df2f278305e.json"
        tts_client = texttospeech.TextToSpeechClient()
        synthesis_input = texttospeech.SynthesisInput(text=text)
        voice = texttospeech.VoiceSelectionParams(
            language_code="ko-KR",
            ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL
        )
        audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

        response = tts_client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)
        audio = io.BytesIO(response.audio_content)
        song = AudioSegment.from_mp3(audio)
        play(song)
        time.sleep(0.1)
        self.is_speaking = False
        self.setup_audio_stream()

    def get_gpt_response(self, text):
        messages = [{"role": "system", "content": "일상 대화"}]
        for question, answer in self.history:
            messages.append({"role": "user", "content": question})
            messages.append({"role": "assistant", "content": answer})
        messages.append({"role": "user", "content": text})

        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages
            )
            return response.choices[0].message["content"].strip()
        except Exception as e:
            return f"An error occurred: {str(e)}"

    def monitor_silence(self):
        while self.running:
            if time.time() - self.last_transcript_time > self.silence_threshold:
                if not self.is_speaking:
                    self.history.clear()
                    self.last_transcript_time = time.time()
            time.sleep(0.25)

transcriber = Transcriber()
transcription_thread = threading.Thread(target=transcriber.transcribe_streaming)
monitor_thread = threading.Thread(target=transcriber.monitor_silence)

transcription_thread.start()
monitor_thread.start()

transcription_thread.join()
monitor_thread.join()
