import os
import time
import threading
import pyaudio
import numpy as np
import configparser
from google.cloud import speech_v1 as speech
from google.cloud import texttospeech
from pydub import AudioSegment
from pydub.playback import play
import io
import openai

# ffmpeg와 ffprobe 경로 직접 설정
os.environ["PATH"] += os.pathsep + "C:\\Users\\wwsgb\\ffmpeg\\ffmpeg-6.1.1-full_build\\bin"

# 설정파일 로드
config = configparser.ConfigParser()
config.read('config.ini')
openai.api_key = config.get('api_keys', 'openai_key')

# OpenAI GPT 응답을 받는 함수
def get_gpt_response(text):
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "user", "content": text}
            ]
        )
        return response.choices[0].message["content"].strip()
    except Exception as e:
        return f"An error occurred: {str(e)}"

def text_to_speech(text):
    # TTS API 인증 정보 설정
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:\\Users\\wwsgb\\Json_key\\tts_json_key\\buoyant-mason-412215-1df2f278305e.json"
    tts_client = texttospeech.TextToSpeechClient()

    synthesis_input = texttospeech.SynthesisInput(text=text)
    voice = texttospeech.VoiceSelectionParams(
        language_code="ko-KR",
        ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL
    )
    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3
    )

    response = tts_client.synthesize_speech(
        input=synthesis_input,
        voice=voice,
        audio_config=audio_config
    )

    # io.BytesIO 객체로 응답 받은 오디오 데이터를 로드
    audio = io.BytesIO(response.audio_content)
    # io.BytesIO 객체를 사용해 AudioSegment 인스턴스 생성
    song = AudioSegment.from_mp3(audio)
    play(song)

class Transcriber:
    def __init__(self):
        # STT API 인증 정보 설정
            os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:\\Users\\wwsgb\\Json_key\\stt_json_key\\buoyant-mason-412215-845850c6db6c.json"
            self.client = speech.SpeechClient()
            self.transcript = ""
            self.last_transcript_time = time.time()
            self.silence_threshold = 0.3
            self.running = True

    def transcribe_streaming(self):
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code="ko-KR",
            metadata=speech.RecognitionMetadata(
                interaction_type=speech.RecognitionMetadata.InteractionType.VOICE_SEARCH,
                microphone_distance=speech.RecognitionMetadata.MicrophoneDistance.NEARFIELD,
                original_media_type=speech.RecognitionMetadata.OriginalMediaType.AUDIO,
                recording_device_type=speech.RecognitionMetadata.RecordingDeviceType.SMARTPHONE,
            )
        )

        streaming_config = speech.StreamingRecognitionConfig(
            config=config,
            interim_results=True,
        )

        p = pyaudio.PyAudio()
        stream = p.open(
            format=pyaudio.paInt16,
            channels=1,
            rate=16000,
            input=True,
            frames_per_buffer=1024,
        )

        def generate_requests():
            while True:
                data = stream.read(1024, exception_on_overflow=False)
                if not data:
                    break
                yield speech.StreamingRecognizeRequest(audio_content=data)

        requests = generate_requests()
        responses = self.client.streaming_recognize(streaming_config, requests)

        for response in responses:
            if not response.results:
                continue
            result = response.results[-1]
            if result.is_final:
                self.transcript = result.alternatives[0].transcript
                self.last_transcript_time = time.time()
            else:
                transcript = result.alternatives[0].transcript
                if "작동 그만해" in transcript:
                    self.running = False
                    print("프로그램 중단")
                    break

        stream.stop_stream()
        stream.close()
        p.terminate()

    def monitor_silence(self):
        # 변환된 텍스트가 있을 때 TTS를 통해 응답
        while self.running:
            if time.time() - self.last_transcript_time > self.silence_threshold:
                if self.transcript:
                    print("변환 텍스트: {}".format(self.transcript))
                    gpt_response = get_gpt_response(self.transcript)
                    print("GPT 응답: {}".format(gpt_response))
                    text_to_speech(gpt_response)  # 응답을 TTS로 변환하여 출력

                    self.transcript = ""
                    self.last_transcript_time = time.time()

                time.sleep(0.3)

transcriber = Transcriber()
transcription_thread = threading.Thread(target=transcriber.transcribe_streaming)
monitor_thread = threading.Thread(target=transcriber.monitor_silence)

transcription_thread.start()
monitor_thread.start()

transcription_thread.join()
monitor_thread.join()
