import os
import io
import time
import pickle
import openai
import threading
import pyaudio
import numpy as np
import configparser
import tensorflow as tf
from pydub import AudioSegment
from pydub.playback import play
from memory_profiler import profile
from google.cloud import texttospeech
from google.cloud import speech_v1 as speech

# ffmpeg와 ffprobe 경로 직접 설정 (환경변수에 추가했는데 잘 되지 않아 직접 설정)
os.environ["PATH"] += os.pathsep + "C:\\Users\\wwsgb\\ffmpeg\\ffmpeg-6.1.1-full_build\\bin"

# 설정파일 로드
config = configparser.ConfigParser()
config.read('config.ini')
openai.api_key = config.get('api_key', 'openai_key')

# 감정분석
class DetectEmotion:
    def __init__(self, model_path, label_encoder_path):
        """
        모델과 라벨 인코더를 로드합니다.
        """
        self.model = tf.keras.models.load_model(model_path)
        with open(label_encoder_path, 'rb') as file:
            self.label_encoder = pickle.load(file)

        # 모든 연산을 float16으로 설정하여 메모리 사용량 감소
        tf.keras.backend.set_floatx('float16')

    #@profile
    def predict(self, texts):
        """
        주어진 텍스트 리스트에 대해 감정을 예측합니다.
        이 함수는 벡터화된 텍스트를 모델에 입력으로 제공하고, 예측된 감정 레이블을 반환합니다.
        """
        # 텍스트 벡터화 (model.layers[1]은 TextVectorization 레이어라고 가정)
        vectorized_texts = self.model.layers[0](texts)  # Input layer를 지나 TextVectorization layer
        predictions = self.model.predict(vectorized_texts, batch_size=1)
        predicted_classes = np.argmax(predictions, axis=1)
        predicted_emotions = self.label_encoder.inverse_transform(predicted_classes)
        return predicted_emotions

# 오디오 스트림 설정, 주변 소음 측정, STT 기능 클래스
class SpeechToText:
    def __init__(self):
        # STT API 인증 정보 설정
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:\\Users\\wwsgb\\Json_key\\stt_json_key\\sigma-kayak-421806-2be227c3a876.json"
        self.client = speech.SpeechClient()       
        self.transcript = ""                            # 곰돌아 빠진 음성 텍스트로 저장
        self.come_transcript = ""                       # 인식된 음성 텍스트로 저장
        self.stream_use = False                         # 오디오 스트림 사용 중인지 나타내는 플래그
        self.noise_level = 0
        self.setup_audio_stream()
    
    def setup_audio_stream(self):   #pyaudio 활용해 새로운 오디오 입력 스트림 설정
        start_time = time.time()  # 시간 측정 시작
        if not self.stream_use:
            self.pyaudio_instance = pyaudio.PyAudio()
            self.stream = self.pyaudio_instance.open(
                format=pyaudio.paInt16,
                channels=1,
                rate=16000,
                input=True,
                frames_per_buffer=1024  #처리 프레임(오디오 샘플 수)
            )
        end_time = time.time()  # 시간 측정 종료
        self.stream_use = True
        print(f"오디오 스트림 설정 완료 시간: {end_time - start_time} 초")
    
    def restart_audio_stream(self):     #기존의 오디오 입력 스트림 중지 -> 스트림 닫고 인스턴스 종료
        if self.stream.is_active():
            self.stream.stop_stream()
        self.stream.close()
        self.pyaudio_instance.terminate()
        self.stream_use = False  # 스트림 중지 시 플래그 업데이트
        print("오디오 스트림 중지")

    def measure_ambient_noise(self, duration=1.0):
        # print("주변 소음 수준 측정 중...")
        if self.stream.is_active():  # 스트림이 활성화되어 있는지 확인
            frames = []
            for _ in range(int(self.stream._rate / self.stream._frames_per_buffer * duration)):
                data = self.stream.read(self.stream._frames_per_buffer)
                frames.append(np.frombuffer(data, dtype=np.int16))
            frame = np.concatenate(frames)
            return np.max(np.abs(frame)) / 32767
        else:
            return 0  # 스트림이 비활성화되어 있으면 0 반환

    def adjust_recognition_sensitivity(self, noise_level):
        if noise_level < 0.1:
            speech_contexts = [speech.SpeechContext(phrases=["주변이 조용합니다."])]
        elif noise_level < 0.3:
            speech_contexts = [speech.SpeechContext(phrases=["주변이 약간 시끄럽습니다."])]
        else:
            speech_contexts = [speech.SpeechContext(phrases=["주변이 시끄럽습니다."])]
        return speech_contexts

    def transcribe_streaming(self):   # 실시간 사용자 음성 텍스트로 변환하는 함수
       while True:
            print("음성 인식을 시작합니다. (텍스트로 변환 중)")
            if not self.stream_use:
                self.setup_audio_stream()

            if self.stream is not None and self.stream.is_active():
                noise_level = self.measure_ambient_noise()
                speech_contexts = self.adjust_recognition_sensitivity(noise_level)

                # 소음 수준에 따라 마이크 입력 볼륨 조절
                if noise_level < 0.1:
                    self.stream.input_volume_float = 1.0  # 조용할 때는 볼륨을 최대로 설정
                elif noise_level < 0.3:
                    self.stream.input_volume_float = 0.7  # 약간 시끄러울 때는 볼륨을 70%로 설정
                else:
                    self.stream.input_volume_float = 0.4  # 시끄러울 때는 볼륨을 40%로 설정
        
                config = speech.RecognitionConfig(
                    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                    sample_rate_hertz=16000,
                    language_code="ko-KR",
                    metadata=speech.RecognitionMetadata(
                        interaction_type=speech.RecognitionMetadata.InteractionType.VOICE_SEARCH, 
                        microphone_distance=speech.RecognitionMetadata.MicrophoneDistance.NEARFIELD,
                        original_media_type=speech.RecognitionMetadata.OriginalMediaType.AUDIO,
                        recording_device_type=speech.RecognitionMetadata.RecordingDeviceType.SMARTPHONE,
                    ),
                    speech_contexts=speech_contexts
                )

                #interim_results=True => 중간 결과도 반환하겠다.
                streaming_config = speech.StreamingRecognitionConfig(config=config, interim_results=True)

                def generate_requests():  #오디오 스트림에서 읽은 데이터를 StreamingRecognizeRequest 객체에 담아 STT API에 반환
                    start_time = time.time()  # 스트림 시작 시간 기록
                    while time.time() - start_time < 300:  # 5분 이하로만 실행
                        data = self.stream.read(1024, exception_on_overflow=False)
                        yield speech.StreamingRecognizeRequest(audio_content=data)

                requests = generate_requests()
                responses = self.client.streaming_recognize(streaming_config, requests) #STT로 전송해 음성인식 수행 (streaming_recognize 메소드로부터 반환된 스트리밍 응답 객체)

                for response in responses:
                    if not response.results:
                        continue

                    result = response.results[-1] #가장 최근 결과 가져옴

                    if result.is_final:
                        transcript = result.alternatives[0].transcript
                        
                        if "작동 그만해" in transcript:
                            self.stream.stop_stream()
                            self.stream.close()
                            self.pyaudio_instance.terminate()
                            return  # 함수 종료
                        
                        if "곰돌아" in transcript or "공돌아" in transcript or "곤도라" in transcript or "곰도라" in transcript or "곰돌 아" in transcript: 
                            self.come_transcript = transcript
                            print("들어온 질문: {}".format(self.come_transcript))
                            clean_transcript = transcript.replace("곰돌아", "").replace("공돌아", "").replace("곤도라", "").replace("곰도라", "").replace("곰돌 아", "").strip()
                            self.transcript = clean_transcript
                            return self.transcript  # 인식된 텍스트 반환
                        else:
                            print("무시된 음성 입력:", transcript)
                    
                self.stream.stop_stream()
                self.stream.close()
                self.pyaudio_instance.terminate()
                time.sleep(0.15)  # 잠시 대기 후 새 스트림 시작

            if not self.stream.is_active():  # 인식 중 스트림이 닫혔는지 확인
                print("음성 인식이 중단되었습니다.")
                time.sleep(0.25)  # 1초 대기 후 다시 시작

            else:
                if self.stream is not None:
                    self.restart_audio_stream()
                print("음성 인식이 중단되었습니다.")
                time.sleep(0.25)

# GPT API와의 통신, 대화 이력 관리, 대화 없을 시 메모리 초기화, 혼자 뭐하세요 기능 클래스
class GPTResponse:
    def __init__(self):
        self.history = []                               # 대화 이력 저장
        self.last_transcript_time = time.time()         # 녹취 마지막 시간
        self.alone_last_transcript_time = time.time()   # 혼자 뭐하세요를 위한 마지막 녹취 시간
        self.silence_threshold = 5 * 60                 # 5분간 대화 없을 시 리셋
        self.alone = 120 * 60                           # 혼자 2시간 동안 말이 없으면 혼자 뭐하세요 기능을 위한 시간 설정
        self.response_timer = None
        self.silence_timer = None
        self.de = DetectEmotion('light_emotion_model', 'label_encoder.pkl')

    def get_gpt_response(self, text):
        messages = [{"role": "system", "content": "일상 대화"}]
        for question, answer in self.history:
            messages.append({"role": "user", "content": question})
            messages.append({"role": "assistant", "content": answer})
        messages.append({"role": "user", "content": text})

        try:
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=messages
            )
            return response.choices[0].message["content"].strip()
        except Exception as e:
            return f"An error occurred: {str(e)}"

    def emotion_detection(self, transcript):
        emotions = self.de.predict([transcript])
        return emotions[0]

    def process_response(self, transcript):
        emotion = self.emotion_detection(transcript)
        print(f"감지된 감정: {emotion}")
        
        if 'angry' in emotion or '분노' in emotion:
            prompt = f"사용자가 화났습니다. 사용자의 질문: {transcript}\n이에 대한 사용자의 마음을 진정시키고 합리적인 답변을 해주세요."
        elif 'disgust' in emotion:
            prompt = f"사용자가 역겨움을 느끼고 있습니다. 사용자의 질문: {transcript}\n이에 대한 이해하고 수용하는 태도로 답변을 해주세요."
        elif 'fear' in emotion or '불안' in emotion:
            prompt = f"사용자가 불안함 느끼고 있습니다. 사용자의 질문: {transcript}\n이에 대한 안정적이고 위로가 되는 답변을 해주세요."
        elif 'happiness' in emotion or '기쁨' in emotion:
            prompt = f"사용자가 기쁨을 느끼고 있습니다. 사용자의 질문: {transcript}\n이에 대한 밝고 긍정적인 답변을 해주세요."
        elif 'neutral' in emotion or '무감정' in emotion:
            prompt = f"사용자의 질문: {transcript}\n이에 대한  명확하고 객관적인 답변을 해주세요."
        elif 'sadness' in emotion or '슬픔' in emotion or '상처' in emotion:
            prompt = f"사용자가 상처받고, 슬퍼하고 있습니다. 사용자의 질문: {transcript}\n이에 대한 위로의 답변을 해주세요."
        elif 'surprise' in emotion or '당황' in emotion: 
            prompt = f"사용자가 당황하고 있습니다. 사용자의 질문: {transcript}\n이에 대한 안정감을 주고 상황을 명확히 해주는 답변을 해주세요."
        else:
            prompt = f"사용자의 질문: {transcript}\n 이에 대한 답변을 해주세요."
        
        gpt_response = self.get_gpt_response(prompt)
        print("GPT 응답: {}".format(gpt_response))
        self.history.append((transcript, gpt_response))
        self.last_transcript_time = time.time()
        self.alone_last_transcript_time = time.time()
        return gpt_response

    def start_response_timer(self):
        if self.response_timer is not None:
            self.response_timer.cancel()
        self.response_timer = threading.Timer(self.alone, self.check_alone)
        self.response_timer.start()

    def start_silence_timer(self):
        if self.silence_timer is not None:
            self.silence_timer.cancel()
        self.silence_timer = threading.Timer(self.silence_threshold, self.reset_memory)
        self.silence_timer.start()

    def reset_memory(self):
        if time.time() - self.last_transcript_time > self.silence_threshold:
            print(f"{self.silence_threshold}초 이상 대화 없어 메모리 초기화합니다.")
            self.history.clear()
            self.last_transcript_time = time.time()
            self.alone_last_transcript_time = time.time()

    def check_alone(self):
        if time.time() - self.alone_last_transcript_time > self.alone:
            print(" 혼자 뭐하세요?")
            self.alone_last_transcript_time = time.time() 
            self.start_response_timer()  # 타이머 재설정

# GPT 응답을 음성으로 변환하는 TTS 기능 클래스
class TextToSpeech:
    def __init__(self):
        self.is_speaking = False                        # GPT 말하고 있는지에 대한 여부

    def text_to_speech(self, text):  # GPT로부터 받은 응답 음성 변환 함수, 마이크 제어 로직 포함
        self.is_speaking = True
        stt = SpeechToText()
        stt.restart_audio_stream()  # 응답 시작 전 마이크 중지 (응답할때 오디오 스트림 재시작을 위한 마이크 중지)
        while stt.stream_use:
            time.sleep(0.1)
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:\\Users\\wwsgb\\Json_key\\tts_json_key\\sigma-kayak-421806-c28bf22ca0f0.json"
        tts_client = texttospeech.TextToSpeechClient()
        synthesis_input = texttospeech.SynthesisInput(text=text)
        voice = texttospeech.VoiceSelectionParams(
            language_code="ko-KR",
            ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL
        )
        audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3)

        response = tts_client.synthesize_speech(input=synthesis_input, voice=voice, audio_config=audio_config)
        audio = io.BytesIO(response.audio_content)
        song = AudioSegment.from_mp3(audio)
        play(song)
        time.sleep(0.2)  # 재생 시간 + 약간의 버퍼 시간
        stt.setup_audio_stream()  # 응답 완료 후 오디오스트림 재시작
        self.is_speaking = False

def main():
    stt = SpeechToText()
    gpt = GPTResponse()
    tts = TextToSpeech()

    while True:
        transcript = stt.transcribe_streaming()
        if transcript:
            gpt_response = gpt.process_response(transcript)
            gpt.start_silence_timer()
            gpt.start_response_timer()
            tts.text_to_speech(gpt_response)

if __name__ == "__main__":
    main()
